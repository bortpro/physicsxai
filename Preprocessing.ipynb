{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f67a6a6",
   "metadata": {},
   "source": [
    "## PDF Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d0061b-8b8c-4353-850f-f19466a0ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = './resources/solving_olympiad_geometry.pdf'\n",
    "DEFAULT_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "# OPTIONAL_MODEL = \"google/gemma-2-2b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21029232-ac5f-42ca-b26b-baad5b2f49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from typing import Optional\n",
    "import os\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153d9ece-37a4-4fff-a8e8-53f923a2b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_pdf(file_path: str) -> bool:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at path: {file_path}\")\n",
    "        return False\n",
    "    if not file_path.lower().endswith('.pdf'):\n",
    "        print(\"Error: File is not a PDF\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a362ac3",
   "metadata": {},
   "source": [
    "Convert PDF to a `.txt` file. This would simply read and dump the contents of the file. We set the maximum characters to 100k. \n",
    "\n",
    "For people converting their favorite novels into a podcast, they will have to add extra logic of going outside the Llama models context length which is 128k tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b57c2d64-3d75-4aeb-b4ee-bd1661286b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path: str, max_chars: int = 100000) -> Optional[str]:\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # Create PDF reader object\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            # Get total number of pages\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"Processing PDF with {num_pages} pages...\")\n",
    "            \n",
    "            extracted_text = []\n",
    "            total_chars = 0\n",
    "            \n",
    "            # Iterate through all pages\n",
    "            for page_num in range(num_pages):\n",
    "                # Extract text from page\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "                \n",
    "                # Check if adding this page's text would exceed the limit\n",
    "                if total_chars + len(text) > max_chars:\n",
    "                    # Only add text up to the limit\n",
    "                    remaining_chars = max_chars - total_chars\n",
    "                    extracted_text.append(text[:remaining_chars])\n",
    "                    print(f\"Reached {max_chars} character limit at page {page_num + 1}\")\n",
    "                    break\n",
    "                \n",
    "                extracted_text.append(text)\n",
    "                total_chars += len(text)\n",
    "                print(f\"Processed page {page_num + 1}/{num_pages}\")\n",
    "            \n",
    "            final_text = '\\n'.join(extracted_text)\n",
    "            print(f\"\\nExtraction complete! Total characters: {len(final_text)}\")\n",
    "            return final_text\n",
    "            \n",
    "    except PyPDF2.PdfReadError:\n",
    "        print(\"Error: Invalid or corrupted PDF file\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023397b",
   "metadata": {},
   "source": [
    "Helper function to grab meta info about our PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0984bb1e-d52c-4cec-a131-67a48061fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PDF metadata\n",
    "def get_pdf_metadata(file_path: str) -> Optional[dict]:\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            metadata = {\n",
    "                'num_pages': len(pdf_reader.pages),\n",
    "                'metadata': pdf_reader.metadata\n",
    "            }\n",
    "            return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metadata: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019affc",
   "metadata": {},
   "source": [
    "Finally, we can run our logic to extract the details from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63848943-79cc-4e21-8396-6eab5df493e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata...\n",
      "\n",
      "PDF Metadata:\n",
      "Number of pages: 21\n",
      "Document info:\n",
      "/Author: Trieu H. Trinh\n",
      "/CreationDate: D:20240126100646+05'30'\n",
      "/Creator: Springer\n",
      "/Keywords: \n",
      "/ModDate: D:20240126100754+05'30'\n",
      "/Subject: Nature, doi:10.1038/s41586-023-06747-5\n",
      "/Title: Solving olympiad geometry without human demonstrations\n",
      "\n",
      "Extracting text...\n",
      "Processing PDF with 21 pages...\n",
      "Processed page 1/21\n",
      "Processed page 2/21\n",
      "Processed page 3/21\n",
      "Processed page 4/21\n",
      "Processed page 5/21\n",
      "Processed page 6/21\n",
      "Processed page 7/21\n",
      "Processed page 8/21\n",
      "Processed page 9/21\n",
      "Processed page 10/21\n",
      "Processed page 11/21\n",
      "Processed page 12/21\n",
      "Processed page 13/21\n",
      "Processed page 14/21\n",
      "Processed page 15/21\n",
      "Processed page 16/21\n",
      "Processed page 17/21\n",
      "Processed page 18/21\n",
      "Processed page 19/21\n",
      "Processed page 20/21\n",
      "Processed page 21/21\n",
      "\n",
      "Extraction complete! Total characters: 75333\n",
      "\n",
      "Preview of extracted text (first 500 characters):\n",
      "--------------------------------------------------\n",
      "476 | Nature | Vol 625 | 18 January 2024\n",
      "ArticleSolving olympiad geometry without human \n",
      "demonstrations\n",
      "Trieu H. Trinh1,2 ✉, Yuhuai Wu1, Quoc V. Le1, He  He2 & Thang Luong1 ✉\n",
      "Proving mathematical theorems at the olympiad level represents a notable milestone \n",
      "in human-level automated reasoning1–4, owing to their reputed difficulty among the \n",
      "world’s best talents in pre-university mathematics. Current machine-learning \n",
      "approaches, however, are not applicable to most mathematical domains owing to t\n",
      "--------------------------------------------------\n",
      "\n",
      "Total characters extracted: 75333\n",
      "\n",
      "Extracted text has been saved to extracted_text.txt\n"
     ]
    }
   ],
   "source": [
    "# Extract metadata first\n",
    "print(\"Extracting metadata...\")\n",
    "metadata = get_pdf_metadata(pdf_path)\n",
    "if metadata:\n",
    "    print(\"\\nPDF Metadata:\")\n",
    "    print(f\"Number of pages: {metadata['num_pages']}\")\n",
    "    print(\"Document info:\")\n",
    "    for key, value in metadata['metadata'].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Extract text\n",
    "print(\"\\nExtracting text...\")\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Display first 500 characters of extracted text as preview\n",
    "if extracted_text:\n",
    "    print(\"\\nPreview of extracted text (first 500 characters):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(extracted_text[:500])\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"\\nTotal characters extracted: {len(extracted_text)}\")\n",
    "\n",
    "# Optional: Save the extracted text to a file\n",
    "if extracted_text:\n",
    "    output_file = 'extracted_text.txt'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(extracted_text)\n",
    "    print(f\"\\nExtracted text has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d1f59",
   "metadata": {},
   "source": [
    "### Llama Pre-Processing\n",
    "\n",
    "Now let's proceed to justify our distaste for writing regex and use that as a justification for a LLM instead:\n",
    "\n",
    "At this point, have a text file extracted from a PDF of a paper. Generally PDF extracts can be messy due to characters, formatting, Latex, Tables, etc. \n",
    "\n",
    "One way to handle this would be using regex, instead we can also prompt the feather light Llama models to clean up our text for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c0828a5-964d-475e-b5f5-40a04e287725",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are a world class text pre-processor, here is the raw data from a PDF, please parse and return it in a way that is crispy and usable to send to a podcast writer.\n",
    "\n",
    "The raw data is messed up with new lines, Latex math and you will see fluff that we can remove completely. Basically take away any details that you think might be useless in a podcast author's transcript.\n",
    "\n",
    "Remember, the podcast could be on any topic whatsoever so the issues listed above are not exhaustive\n",
    "\n",
    "Please be smart with what you remove and be creative ok?\n",
    "\n",
    "Remember DO NOT START SUMMARIZING THIS, YOU ARE ONLY CLEANING UP THE TEXT AND RE-WRITING WHEN NEEDED\n",
    "\n",
    "Be very smart and aggressive with removing details, you will get a running portion of the text and keep returning the processed text.\n",
    "\n",
    "PLEASE DO NOT ADD MARKDOWN FORMATTING, STOP ADDING SPECIAL CHARACTERS THAT MARKDOWN CAPATILISATION ETC LIKES\n",
    "\n",
    "ALWAYS start your response directly with processed text and NO ACKNOWLEDGEMENTS about my questions ok?\n",
    "Here is the text:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd393fae",
   "metadata": {},
   "source": [
    "Instead of having the model process the entire file at once, as you noticed in the prompt-we will pass chunks of the file. \n",
    "\n",
    "One issue with passing chunks counted by characters is, we lose meaning of words so instead we chunk by words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e8a547-9d7c-4e2f-be9e-a3aea09cce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_bounded_chunks(text, target_chunk_size):\n",
    "    \"\"\"\n",
    "    Split text into chunks at word boundaries close to the target chunk size.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        word_length = len(word) + 1  # +1 for the space\n",
    "        if current_length + word_length > target_chunk_size and current_chunk:\n",
    "            # Join the current chunk and add it to chunks\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = word_length\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_length += word_length\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74223f",
   "metadata": {},
   "source": [
    "Let's load in the model and start processing the text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d04a4f07-b0b3-45ca-8f41-a433e1abe050",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    device_map=device,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "model, tokenizer = accelerator.prepare(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0205839-2eaa-426c-817a-d879c669c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes sure everything runs on CPU and doesn't get split on GPU for Gemma\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbda5241-e890-4402-87dd-514d6761bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(text_chunk, chunk_num):\n",
    "    \"\"\"Process a chunk of text and return both input and output for verification\"\"\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": text_chunk},\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            max_new_tokens=512\n",
    "        )\n",
    "    \n",
    "    processed_text = tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt):].strip()\n",
    "    \n",
    "    # Print chunk information for monitoring\n",
    "    #print(f\"\\n{'='*40} Chunk {chunk_num} {'='*40}\")\n",
    "    print(f\"INPUT TEXT:\\n{text_chunk[:500]}...\")  # Show first 500 chars of input\n",
    "    print(f\"\\nPROCESSED TEXT:\\n{processed_text[:500]}...\")  # Show first 500 chars of output\n",
    "    print(f\"{'='*90}\\n\")\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0183c47-339d-4041-ae83-77fc34931075",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"./resources/extracted_text.txt\"  # Replace with file path\n",
    "CHUNK_SIZE = 1000  # Adjust chunk size if needed\n",
    "\n",
    "chunks = create_word_bounded_chunks(extracted_text, CHUNK_SIZE) # fixed bug\n",
    "num_chunks = len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb36814f-9310-4734-bf54-e16a5032339e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "447188d3-ebf0-42d5-940e-4d7e0d9dbf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Calculate number of chunks\n",
    "num_chunks = (len(text) + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
    "\n",
    "# Create output file name\n",
    "output_file = f\"clean_{os.path.basename(INPUT_FILE)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7917dfdd-b3af-44fc-a8c0-2760ace9363e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df04c68db0524811a6e141327bee53d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chunks:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "476 | Nature | Vol 625 | 18 January 2024 ArticleSolving olympiad geometry without human demonstrations Trieu H. Trinh1,2 ✉, Yuhuai Wu1, Quoc V. Le1, He He2 & Thang Luong1 ✉ Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning1–4, owing to their reputed difficulty among the world’s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the hi...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ng, owing to the difficulty among world's best talents in pre-university mathematics Current machine-learning approaches are not applicable to most mathematical domains due to high cost of translating human proofs into machine-verifiable format The problem is worse for geometry due to unique translation challenges resulting in scarcity of training data We propose AlphaGeometry a theorem prover for Euclidean plane geometry that synthesizes millions of theorems and proofs across different levels o...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 un...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "anching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004. Pr...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "theorem-proving competitions in the world, with a similarly long history dating back to 1959, playing an instrumental role in identifying exceptional talents in problem solving. Matching top human performances at the olympiad level has become a notable milestone of AI research2–4. Theorem proving is difficult for learning-based methods because training data of human proofs translated into machine-verifiable lan- guages are scarce in most mathematical domains. Geometry stands out among other olym...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "n identifying exceptional talents in problem-solving. \n",
      "\n",
      "Theorem proving is challenging for learning-based methods due to the scarcity of machine-verifiable proofs in most mathematical domains, particularly in geometry. Geometry stands out as an exception due to its unique characteristics, such as having few proof options in general-purpose languages like Lean, whereas geometry-specific languages are narrowly defined, limiting their expressiveness, and often relying on tools beyond the scope of g...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "demonstrations2–4. Current approaches to geometry, therefore, still primarily rely on symbolic methods and human-designed, hard-coded search heuristics10–14. We present an alternative method for theorem proving using syn - thetic data, thus sidestepping the need for translating human-provided proof examples. We focus on Euclidean plane geometry and exclude topics such as geometric inequalities and combinatorial geometry. By using existing symbolic engines on a diverse set of random theo - rem pr...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "igned, hard-coded search heuristics10–14. We present an alternative method for theorem proving using synthetic data, thus sidestepping the need for translating human-provided proof examples. We focus on Euclidean plane geometry and exclude topics such as geometric inequalities and combinatorial geometry. By using existing symbolic engines on a diverse set of random premises, we extracted 100 million synthetic theorems and their proofs, many with more than 200 proof steps, four times longer than ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "generation, representing the infinite branching fac - tor of theorem proving, and widely recognized in other mathematical domains as the key challenge to proving many hard theorems1,2. Our work therefore demonstrates a successful case of generating synthetic data and learning to solve this key challenge. With this solution, we present a general guiding framework and discuss its applicability to other domains in Methods section ‘ AlphaGeometry framework and applicability to other domains’ . We pr...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "orem proving, widely recognized in other mathematical domains as the key challenge to proving many hard theorems. Our work demonstrates a successful case of generating synthetic data and learning to solve this key challenge. With this solution, we present a general guiding framework and discuss its applicability to other domains in the Methods section. We pretrain a language model on all generated synthetic data and fine-tune it to focus on auxiliary construction during proof search, delegating ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "geometry theorem prover AlphaGeometry, illustrated in Fig. 1, produces human-readable proofs, substantially outperforms the previous state-of-the-art geometry-theorem-proving computer program and approaches the performance of an average IMO gold https://doi.org/10.1038/s41586-023-06747-5 Received: 30 April 2023 Accepted: 13 October 2023 Published online: 17 January 2024 Open access Check for updates 1Google Deepmind, Mountain View, CA, USA. 2Computer Science Department, New York University, New ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "tantially outperforms the previous state-of-the-art geometry-theorem-proving computer program and approaches the performance of an average IMO gold, received 30 April 2023, accepted 13 October 2023, published online 17 January 2024, open access, check for updates, Google Deepmind, Mountain View, CA, USA, 2Computer Science Department, New York University, New York, NY, USA, thangluong@google.com...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "found in Extended Data Table 1. In our work, we sampled nearly 1 billion of such premises in a highly parallelized setting, described in Methods. Note that we do not make use of any existing theorem premises from human-designed problem sets and sampled the eligible constructions uniformly randomly. Next we use a symbolic deduction engine on the sampled prem - ises. The engine quickly deduces new true statements by following forward inference rules as shown in Fig. 3b . This returns a directed ac...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "emises are uniformly randomly selected from existing problem sets. A symbolic deduction engine is used to deduce new true statements following forward inference rules, as shown in Fig. 3b. This engine quickly deduces statements by connecting nodes in a directed acyclic graph, with edges tracing back to their parents, thanks to the traceback algorithm. This process allows for a recursive tracing from any node N, ultimately returning a dependency graph (N), with N as its root and a subset of sampl...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "conclusion, proof) = (P , N, G(N)). In geometry, the symbolic deduction engine is deductive database (refs. 10 ,17), with the ability to efficiently deduce new statements from the premises by means of geometric rules. DD follows deduction rules in the form of definite Horn clauses, that is, Q (x) ← P1(x),…, Pk(x), in which x are points objects, whereas P1,…, Pk and Q are predicates such as ‘equal segments’ or ‘collinear’ . A full list of deduction rules Not solvedConstruct cLanguage modelSolutio...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "fficiently deduce new statements from the premises by means of geometric rules. DD follows deduction rules in the form of definite Horn clauses, where points objects are represented by predicates such as ‘equal segments’ or ‘collinear’. A full list of deduction rules is not provided, but a solution is given for the problem of proving that in an acute triangle ABC, the circumcircles of triangles FKM and KQH are tangent to each other....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "bdAlphaGeometry e Alpha- GeometryA C B D C F MHKA Q O1 OOOO2 A BC Solutionf AAA B M FCKQQQ O2 O1 OOO HHH DDEE GGG Fig. 1 | Overview of our neuro-symbolic AlphaGeometry and how it solves both a simple problem and the IMO 2015 Problem 3. The top row shows how AlphaGeometry solves a simple problem. a , The simple example and its diagram. b, AlphaGeometry initiates the proof search by running the symbolic deduction engine. The engine exhaustively deduces new statements from the theorem premises unti...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ry solves the problem. A, The simple example and its diagram. B, AlphaGeometry initiates the proof search by running the symbolic deduction engine. The engine exhaustively deduces new statements from the theorem premises until the theorem is proven or new statements are exhausted. C, Because the symbolic engine fails to find a proof, the language model constructs one auxiliary point, growing the proof state before the symbolic engine retries. The loop continues until a solution is found. D, For ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "collinear”, highlighted in blue. The bottom row shows how AlphaGeometry solves the IMO 2015 Problem 3 (IMO 2015 P3). e , The IMO 2015 P3 problem statement and diagram. f , The solution of IMO 2015 P3 has three auxiliary points. In both solutions, we arrange language model outputs (blue) interleaved with symbolic engine outputs to reflect their execution order. Note that the proof for IMO 2015 P3 in f is greatly shortened and edited for illustration purposes. Its full version is in the Supplement...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ment and diagram\n",
      "The solution of IMO 2015 P3 has three auxiliary points\n",
      "In both solutions, we arrange language model outputs (blue) interleaved with symbolic engine outputs to reflect their execution order\n",
      "\n",
      "**Note:** The proof for IMO 2015 P3 in this image is greatly shortened and edited for illustration purposes. Its full version is in the Supplementary Information\n",
      "\n",
      "**Number of solved problems**\n",
      "10.019.322.925.025.9\n",
      "15.230\n",
      "20\n",
      "10\n",
      "0\n",
      "\n",
      "**Number of solved problems in IMO-AG-30**\n",
      "Average IMO contesta...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "geometry environment used in our work. Human performance is estimated by rescaling their IMO contest scores between 0 and 7 to between 0 and 1, to match the binary outcome of failure/success of the machines. For example, a contestant’s score of 4 out of 7 will be scaled to 0.57 problems in this comparison. On the other hand, the score for AlphaGeometry and other machine solvers on any problem is either 0 (not solved) or 1 (solved). Note that this is only an approximate comparison with humans on ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "g their IMO contest scores between 0 and 7 to match the binary outcome of machine solvers. For example, a contestant’s score of 4 out of 7 is scaled to 0.57 problems. The score for AlphaGeometry and other machine solvers on any problem is either 0 (not solved) or 1 (solved). This is an approximate comparison with humans on classical geometry, who operate on natural-language statements rather than domain-specific translations. The general IMO contest also includes other types of problems, such as...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "component to the symbolic engine that can deduce new statements through algebraic rules (AR), as described in Methods. AR is necessary to perform angle, ratio and distance chasing, as often required in many olympiad- level proofs. We included concrete examples of AR in Extended Data Table 2. The combination DD + AR, which includes both their for - ward deduction and traceback algorithms, is a new contribution in our work and represents a new state of the art in symbolic reasoning in geometry. Ge...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "escribed in Methods, AR is necessary to perform angle, ratio and distance chasing, as often required in many olympiad-level proofs.\n",
      "\n",
      "We included concrete examples of AR in Extended Data Table 2.\n",
      "\n",
      "The combination DD + AR, which includes both forward deduction and traceback algorithms, is a new contribution in our work and represents a new state of the art in symbolic reasoning in geometry.\n",
      "\n",
      "Generating proofs beyond symbolic deduction So far, the generated proofs consist purely of deduction steps ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "objects. We move this dif- ference from P to the proof so that a generative model that learns to generate the proof can learn to construct them, as illustrated in Fig. 3c. Such proof steps perform auxiliary constructions that symbolic deduc - tion engines are not designed to do. In the general theorem-proving context, auxiliary construction is an instance of exogenous term gen- eration, a notable challenge to all proof-search algorithms because it introduces infinite branching points to the sear...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "generate the proof can learn to construct them, as illustrated in Fig. 3c. Such proof steps perform symbolic deduction engines are not designed to do. In general theorem-proving context, auxiliary constructions are an instance of exogenous term generation, a notable challenge to all proof-search algorithms because it introduces infinite branching points to the search tree. In geometry theorem proving, auxiliary constructions are the longest-standing subject of study since inception of the field ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "demonstrations. Training a language model on synthetic data The transformer18 language model is a powerful deep neural network that learns to generate text sequences through next-token predic - tion, powering substantial advances in generative AI technology. We serialize (P , N, G(N)) into a text string with the structure ‘<premises> <conclusion><proof>’ . By training on such sequences of symbols, a language model effectively learns to generate the proof, conditioning on theorem premises and con...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "former18 language model is a powerful deep neural network that learns to generate text sequences through next-token prediction, powering substantial advances in generative AI technology. We serialize (P, N, G(N)) into a text string with the structure <premises> <conclusion><proof> By training on such sequences of symbols, a language model effectively learns to generate the proof, conditioning on theorem premises and conclusion. Combining language modeling and symbolic engines, proof search is a ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "describing one new auxiliary construction such as “construct point X so that ABCX is a parallelogram” . Each time the language model generates one such construction, the symbolic engine is provided with new inputs to work with and, therefore, its deduction closure expands, potentially reaching the conclusion. We use beam search to explore the top k constructions generated by the language model and describe the parallelization of this proof-search algorithm in Methods. Empirical evaluation An oly...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "nstruct point Z so that BXY is a trapezoid\n",
      "\n",
      "Construct point W so that CXY is a rhombus...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "cyclic(E,B,C,D)…Synthetic problems and proofs … …… ab c /uni2220EAH = /uni2220EDH CC FF BBHHEEGGDDAA CC BBHHEEEDDDAAABBEEDD CCDD EEAA Fig. 3 | AlphaGeometry synthetic-data-generation process. a, We first sample a large set of random theorem premises. b , We use the symbolic deduction engine to obtain a deduction closure. This returns a directed acyclic graph of statements. For each node in the graph, we perform traceback to find its minimal set of necessary premise and dependency deductions. For...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "CC BBHHEEEDDDAAABBEEDD CCDD EEAA Fig. 3 | AlphaGeometry synthetic-data-generation process. a, We first sample a large set of random theorem premises. b, We use the symbolic deduction engine to obtain a deduction closure. This returns a directed acyclic graph of statements. For each node in the graph, we perform traceback to find its minimal set of necessary premise and dependency deductions. For the rightmost node ‘HA ⊥ BC’, traceback returns the green subgraph. c, The minimal premise and the co...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "geometry problems from the IMO competitions since 2000 to a narrower, specialized environment for classical geometry used in interactive graphical proof assistants13,17,19, as discussed in Methods. Among all non-combinatorial geometry-related problems, 75% can be represented, resulting in a test set of 30 classical geometry problems. Geometric inequality and combinatorial geometry, for example, can- not be translated, as their formulation is markedly different to classical geometry. We include t...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "cialized environment for classical geometry used in interactive graphical proof assistants. Non-combinatorial geometry-related problems can be represented, resulting in a test set of 30 classical geometry problems. This test set is named IMO-AG-30, highlighting its source, method of translation, and current size. The final test set includes the full list of statements and translations for all 30 problems. Geometry theorem provers in the literature fall into two categories. The first category is ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "of large polynomials. Gröbner bases20 and Wu’s method21 are representative approaches in this category, with theoretical guarantees to success- fully decide the truth value of all geometry theorems in IMO-AG-30, albeit without a human-readable proof. Because these methods often have large time and memory complexity, especially when processing IMO-sized problems, we report their result by assigning success to any problem that can be decided within 48 h using one of their existing implementations1...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "th theoretical guarantees to success for deciding the truth value of all geometry theorems in IMO-AG-30, albeit without a human-readable proof.\n",
      "\n",
      "These methods often have large time and memory complexity, especially when processing IMO-sized problems, we report their results by assigning success to any problem that can be decided within 48 hours using one of their existing implementations.\n",
      "\n",
      "AlphaGeometry belongs to the second category of solvers, often described as search/axiomatic or sometimes's...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "as “If OA ⊥ OB and OA = OB, construct C on the oppo - site ray of OA such that OC = OA” , besides 75 deduction rules for the symbolic engine. Large language models22–24 such as GPT-4 (ref. 25 ) can be considered to be in this category. Large language models have demonstrated remarkable reasoning ability on a variety of reasoning tasks26–29. When producing full natural-language proofs on IMO-AG-30, however, GPT-4 has a success rate of 0%, often making syntactic and semantic errors throughout its ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "known theorems and beyond We find that our synthetic data generation can rediscover some fairly complex theorems and lemmas known to the geometry literature, as shown in Fig. 4 , despite starting from randomly sampled theorem IMO 2015 P3 AlphaGeometry proof length: 112 0.001% data 107 103 101 0.05% data Average IMO proof length from AlphaGeometry Trivial theorem Proof length = 1Well-known theorem Proof length = 20Most complex synthetic theorem Proof length = 247 105 Count (log scale) IMO 2019 P6...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "plex theorems and lemmas known to the geometry literature, as shown in Fig. 4. Despite starting from randomly sampled theorem. 0.001% of data. 107 103 101 0.05% of data. Average IMO proof length from AlphaGeometry Trivial theorem Proof length = 1. Well-known theorem Proof length = 20. Most complex synthetic theorem Proof length = 247. Count (log scale) IMO 2019 P6 AlphaGeometry proof length: 187. With auxiliary construction. Pure deduction....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "synthetic proof has an impressive length of 247 with two auxiliary constructions. Most synthetic theorem premises tend not to be symmetrical like human-discovered theorems, as they are not biased towards any aesthetic standard. 480 | Nature | Vol 625 | 18 January 2024 Articlepremises. This can be attributed to the use of composite actions described in Extended Data Table 1, such as ‘taking centroid’ or ‘tak - ing excentre’ , which—by chance—sampled a superset of well-known theorem premises, unde...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "uxiliary constructions. Most synthetic theorem premises tend not to be symmetrical like human-discovered theorems, as they are not biased towards any aesthetic standard. This is attributed to the use of composite actions described in Extended Data Table 1, such as 'taking centroid' or 'taking excentre', which sampled a superset of well-known theorem premises, under our large-scale exploration setting described in Methods. To study the complexity of synthetic proofs, Fig. 4 shows a histogram of s...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "aesthetic biases such as being symmetrical, therefore covering a wider set of scenarios known to Euclidean geom- etry. We performed deduplication as described in Methods, resulting in more than 100 millions unique theorems and proofs, and did not find any IMO-AG-30 theorems, showing that the space of possible geometry theorems is still much larger than our discovered set. Language model pretraining and fine-tuning We first pretrained the language model on all 100 million synthetically generated ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "d of geometry has a wide range of potential applications....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "prompting GPT-4 to produce full proofs in natural language with several rounds of reflections and revisions, we also combine GPT-4 with DD + AR as another baseline to enhance its deduction accuracy. To achieve this, we use detailed instructions and few-shot examples in the prompt to help GPT-4 successfully interface with DD + AR, providing auxiliary constructions in the correct gram - mar. Prompting details of baselines involving GPT-4 is included in the Supplementary Information. AlphaGeometry ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "t examples provided in the prompt to interface with DD + AR, and use auxiliary constructions in the correct grammar. Prompting details of the baselines involving AlphaGeometry are included in the Supplementary Information. The previous state of the art (Wu's method) solved ten problems, whereas the strongest baseline (DD + AR + human-designed heuristics) solved 18 problems, utilizing the algebraic reasoning engine developed in this work and the human heuristics designed by Chou et al. To match t...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "auxiliary constructions suggested by human-designed heuristics in parallel, until success or timeout. Other baselines such as Wu’s method or the full-angle method are not affected by parallel compute resources as they carry out fixed, step-by-step algorithms until termination. Measuring the improvements made on top of the base symbolic deduction engine (DD), we found that incorporating algebraic deduc - tion added seven solved problems to a total of 14 (DD + AR), whereas the language model’s aux...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ut. Other baselines such as Wu’s method or the full-angle method are not affected by parallel compute resources as they carry out fixed, step-by-step algorithms until termination. Measuring the improvements made on top of the base symbolic deduction engine (DD), we found that incorporating algebraic deduction added seven solved problems to a total of 14 (DD + AR), whereas the language model’s auxiliary construction remarkably added another 11 solved problems, resulting in a total of 25. As repor...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "regional olympiads and famous theorems, we find that baselines in Table 1 remain at the same performance rankings, with AlphaGeometry solving almost all problems (98.7%), whereas Wu’s method solved 75% and DD + AR + human-designed heuristics solved 92.2%, as reported in Extended Data Fig. 6b. Notably, AlphaGeometry solved both geometry problems of the same year in 2000 and 2015, a threshold widely considered difficult to the average human contestant at the IMO. Further, the traceback process of ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "l problems (98.7%), whereas Wu's method solved 75% and DD + AR + human-designed heuristics solved 92.2%, as reported in Extended Data Fig. 6b. Notably, AlphaGeometry solved both geometry problems of the same year in 2000 and 2015, a threshold widely considered difficult to the average human contestant at the IMO. Further, the traceback process of AlphaGeometry found an unused premise in the translated IMO 2004 P1, as shown in Fig. 5, therefore discovering a more general version of the translated...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "of the synthetic data, test-time performance and proof readability. Human expert evaluation of AlphaGeometry outputs Because AlphaGeometry outputs highly interpretable proofs, we used a simple template to automatically translate its solutions to natural language. To obtain an expert evaluation in 2000 and 2015, during which AlphaGeometry solves all geometry problems and potentially passes the medal threshold, we submit these solutions to the USA IMO team coach, who is experienced in grading math...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "an expert evaluation of AlphaGeometry outputs highly interpretable proofs, using a simple template to automatically translate its solutions to natural language. To obtain an expert evaluation in 2000 and 2015, during which AlphaGeometry solves all geometry problems and potentially passes the medal threshold, we submit these solutions to the USA IMO team coach, who is experienced in grading mathematical olympiads and has authored books on olympiad geometry training. AlphaGeometry solutions are re...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "report the results in Extended Data Fig. 1. Learning to predict the symbolic engine’s output improves the language model’s auxiliary construction In principle, auxiliary construction strategies must depend on the details of the specific deduction engine they work with during proof search. We find that a language model without pretraining only solves 21 problems. This suggests that pretraining on pure deduction proofs generated by the symbolic engine DD + AR improves the suc - cess rate of auxili...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ion In principle, auxiliary construction strategies must depend on the details of the specific deduction engine they work with during proof search. We find that a language model without pretraining only solves 21 problems. This suggests that pretraining on pure deduction proofs generated by the symbolic engine DD + AR improves the success rate of auxiliary constructions. On the other hand, a language model without fine-tuning also degrades the performance but not as severely, with 23 problems so...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "(ours) 14 DD + AR + GPT-4 auxiliary constructions15 DD + AR + human-designed heuristics 18 AlphaGeometry 25 • Without pretraining 21 • Without fine-tuning 23 We compare AlphaGeometry to other state-of-the-art methods (computer algebra and search approaches), most notably Wu’s method. We also show the results of DD + AR (our contribution) and its variants, resulting in the strongest baseline DD + AR + human-designed heuristics. Finally, we include ablation settings for AlphaGeometry without pretr...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "metry 25 • Without pretraining 21 Without fine-tuning 23 We compare AlphaGeometry to other state-of-the-art methods (computer algebra and search approaches), most notably Wu’s method. We also show the results of DD + AR (our contribution) and its variants, resulting in the strongest baseline DD + AR + human-designed heuristics. Finally, we include ablation settings for AlphaGeometry without pretraining and fine-tuning. | Nature | Vol 625 | 18 January 2024 | 481Hard problems are reflected in Alph...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "easier problems (average human score > 3.5), however, we observe no correlation (p = −0.06) between the average human score and AlphaGeometry proof length. Conclusion AlphaGeometry is the first computer program to surpass the per - formance of the average IMO contestant in proving Euclidean plane geometry theorems, outperforming strong computer algebra and search baselines. Notably, we demonstrated through AlphaGeometry a neuro-symbolic approach for theorem proving by means of large-scale explor...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "n the average human score and AlphaGeometry proof length. Conclusion AlphaGeometry is the first computer program to surpass the performance of the average IMO contestant in proving Euclidean plane geometry theorems, outperforming strong computer algebra and search baselines. Notably, we demonstrated through AlphaGeometry a neuro-symbolic approach for theorem proving by means of large-scale exploration from scratch, sidestepping the need for human-annotated proof examples and human-curated proble...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "triangles BMR and CNR have a common point lying on the side BC.” Translate SolveGeneralizePremise ProofTraceback Unused premise Used premises Neural net output Symbolic solver outputRRR MMM NNN BBB PPP CCCAAA OOO RRR NNN MMM AAA CCC BBB PPPOOO Fig. 5 | AlphaGeometry discovers a more general theorem than the translated IMO 2004 P1. Left, top to bottom, the IMO 2004 P1 stated in natural language, its translated statement and AlphaGeometry solution. Thanks to the traceback algorithm necessary to ex...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "TracebackUnused premise Used premises Neural net output Symbolic solver outputRRAfter analyzing the problem, it becomes clear that the condition for collinearity is not as straightforward as initially stated. Fig. 5 | AlphaGeometry discovers a more general theorem than the translated IMO 2004 P1. Left, top to bottom, the IMO 2004 P1 stated in natural language, its translated statement and AlphaGeometry solution. Thanks to the traceback algorithm necessary to extract the minimal premises, AlphaGe...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "Average score of IMO human contestantsNeed construction Deduction onlyDif/f_iculty of IMO problems for AlphaGeometry versus humans Harder for AlphaGeometry Harder for humans2019 P6 2000 P6 2015 P3 2009 P2 2010 P2 2007 P42004 P12012 P5 1 23 45 67050100150200 Fig. 6 | AlphaGeometry proof length versus the average score of IMO participants on different problems. Among the solved problems, 2000 P6, 2015 P3 and 2019 P6 are the hardest for IMO participants. They also require the longest proofs from Al...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "for AlphaGeometry versus humans Harder for AlphaGeometry Harder for humans2019 P6 2000 P6 2015 P3 2009 P2 2010 P2 2007 P42004 P12012 P5 1 23 45 67050100150200 Fig. 6 | AlphaGeometry proof length versus the average score of IMO participants on different problems. Among the solved problems, 2000 P6, 2015 P3 and 2019 P6 are the hardest for IMO participants. They also require the longest proofs from AlphaGeometry. For easier problems, however, there is little correlation between AlphaGeometry proof ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "and competing interests; and statements of data and code availability are available at https://doi.org/10.1038/s41586-023-06747-5 . 1. Zheng, K., Han, J. M. & Polu, S. MiniF2F: a cross-system benchmark for formal olympiad- level mathematics. Preprint at https://doi.org/10.48550/arXiv.2109.00110 (2022). 2. Polu, S. et al. Formal mathematics statement curriculum learning. Preprint at https:// doi.org/10.48550/arXiv.2202.01344 (2023). 3. Lample, G. et al. Hypertree proof search for neural theorem p...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "org/10.1038/s41586-023-06747-5 \n",
      "\n",
      "1. MiniF2F: a cross-system benchmark for formal olympiad-level mathematics \n",
      "\n",
      "2. Formal mathematics statement curriculum learning \n",
      "\n",
      "3. Hypertree proof search for neural theorem proving \n",
      "\n",
      "4. Proc. 13th International Conference on Artificial General Intelligence, AGI 2020 \n",
      "\n",
      "5. Formalizing IMO problems and solutions in Isabelle/HOL \n",
      "\n",
      "6. in Proc. First International Conference on Information Processing...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "Gelernter, H., Hansen, J. R. & Loveland, D. W. in Papers presented at the May 3–5, 1960, western joint IRE-AIEE-ACM computer conference 143–149 (ACM, 1960). 8. Harrison, J., Urban, J. & Wiedijk, F. in Handbook of the History of Logic Vol. 9 (ed. Siekmann, J. H.) 135–214 (North Holland, 2014). 9. van Doorn, F., Ebner, G. & Lewis, R. Y. in Proc. 13th International Conference on Intelligent Computer Mathematics, CICM 2020 (eds Benzmüller, C. & Miller, B.) 251–267 (Springer, 2020). 10. Chou, S. C., ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "oint IRE-AIEE-ACM computer conference 143–149 (ACM, 1960) 8. Harrison, J., Urban, J. & Wiedijk, F. in Handbook of the History of Logic Vol. 9 (ed. Siekmann, J. H.) 135–214 (North Holland, 2014) 9. van Doorn, F., Ebner, G. & Lewis, R. Y. in Proc. 13th International Conference on Intelligent Computer Mathematics, CICM 2020 (eds Benzmüller, C. & Miller, B.) 251–267 (Springer, 2020) 10. Chou, S. C., Gao, X. S. & Zhang, J. Z. A deductive database approach to automated geometry theorem proving and dis...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "2002 (ed. Winkler, F.) 44–66 (Springer, 2004). 14. Zhou, M. & Yu, X. in Proc. 2nd International Conference on Artificial Intelligence in Education: Emerging Technologies, Models and Applications, AIET 2021 (eds Cheng, E. C. K., Koul, R. B., Wang, T. & Yu, X.) 151–161 (Springer, 2022). 15. Polu, S. & Sutskever, I. Generative language modeling for automated theorem proving. Preprint at https://arxiv.org/abs/2009.03393 (2020).16. Han, J. M., Rute, J., Wu, Y., Ayers, E. W., & Polu, S. Proof artifact...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ial Intelligence in Education: Emerging Technologies Models and Applications AIET 2021 (eds Cheng E C K Koul R B Wang T Yu X) 151–161 Springer 15 Polu S Sutskever I Generative language modeling for automated theorem proving Preprint at https//arxiv org abs 2009.03393 2020 Han J M Rute J Wu Y Ayers E W Polu S Proof artifact co-training for theorem proving with language models Preprint at https//doi org/10 48550 arXiv 2102 06203 17 Ye Z C Gao X S in Proc Automated Deduction in Geometry 7th Interna...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "J., Davenport, J., Joswig, M. & de Wolff, T.) 263–271 (Springer, 2020). 20. Bose, N. K. in Multidimensional Systems Theory and Applications 89–127 (Springer, 1995). 21. Wu, W.-T. On the decision problem and the mechanization of theorem-proving in elementary geometry. Sci. Sin. 21, 159–172 (1978). 22. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving language understanding by generative pre-training. Preprint at https://paperswithcode.com/paper/improving- language-understanding-...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "78. On the decision problem and the mechanization of theorem-proving in elementary geometry. Sci. Sin. 21, 159–172....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "Adv. Neural Inf. Process. Syst. 35, 3843–3857 (2022). 27. Liang, P. et al. Holistic evaluation of language models. Transact. Mach. Learn. Res. https:// doi.org/10.48550/arXiv.2211.09110 (2023). 28. Srivastava, A. et al. Beyond the imitation game: quantifying and extrapolating the capabilities of language models. Transact. Mach. Learn. Res. https://doi.org/10.48550/ arXiv.2206.04615 (2023). 29. Wei, J. et al. Emergent abilities of large language models. Transact. Mach. Learn. Res. https://doi.org...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "f language models.** Transact. Mach. Learn. Res. https://doi.org/10.48550/arXiv.2211.09110 (2023)\n",
      "\n",
      "**Srivastava, A. et al. Beyond the imitation game: quantifying and extrapolating the capabilities of language models.** Transact. Mach. Learn. Res. https://doi.org/10.48550/ arXiv.2206.04615 (2023)\n",
      "\n",
      "**Wei, J. et al. Emergent abilities of large language models.** Transact. Mach. Learn. Res. https://doi.org/10.48550/arXiv.2206.07682 (2022)\n",
      "\n",
      "**Chou, S. C., Gao, X. S. & Zhang, J. Z. Automated generatio...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitte...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "he original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made.\n",
      "\n",
      "Images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly f...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "expertise and substantial research outside the scope of theorem-proving methodologies. To sidestep this barrier, we instead adopted a more specialized language used in GEX10, JGEX17, MMP/ Geometer13 and GeoLogic19, a line of work that aims to provide a logi- cal and graphical environment for synthetic geometry theorems with human-like non-degeneracy and topological assumptions. Examples of this language are shown in Fig. 1d,f . Owing to its narrow formulation, 75% of all IMO geometry problems ca...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "orem-proving methodologies, we sidestep this barrier by adopting a specialized language used in GEX10, JGEX17, MMP/GeoLogic19, a line of work that provides a logical and graphical environment for synthetic theorems with human-like non-degeneracy and topological assumptions.\n",
      "\n",
      "Examples of this language are shown in Fig. 1d,f. Due to its narrow formulation, 75% of all IMO geometry problems can be adapted to this representation. Each proof step is logically and numerically verified and can be evalua...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "as it is a separate and extremely challenging research topic that demands substantial investment from the mathematical formalization community. Sampling consistent theorem premises We developed a constructive diagram builder language similar to that used by JGEX17 to construct one object in the premise at a time, instead of freely sampling many premises that involve several objects, there- fore avoiding the generation of a self-contradicting set of premises. An exhaustive list of construction ac...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "r to that used by jgex17 to construct one object in the premise at a time, instead of freely sampling many premises that involve several objects, to avoid generating a self-contradicting set of premises. an exhaustive list of construction actions is shown in extended data table 1. these actions include creating new points that are related to others in a certain way, such as collinear, incentre, or centre, as well as constructions that take a number as a parameter, for example, construct point X ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "coverage. A more general and expressive diagram builder language can be found in ref. 32 . We make use of a simpler language that is sufficient to describe problems in IMO-AG-30 and can work well with the symbolic engine DD. The symbolic deduction engine The core functionality of the engine is deducing new true statements given the theorem premises. Deduction can be performed by means of geometric rules such as ‘If X then Y’ , in which X and Y are sets of geo - metric statements such as ‘ A, B, ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "of a simpler language that is sufficient to describe problems in IMO-AG-30 and can work well with the symbolic engine DD. The symbolic deduction engine \n",
      "core functionality is deducing new true statements given the theorem premises. Deduction can be performed by means of geometric rules such as ‘If X then Y’, in which X and Y are sets of geo-metric statements such as ‘ A, B, C are collinear’. We use the method of structured DD10,17 for this purpose as it can find the deduction closure in just sec...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "the Gaussian elimination process implemented in GeoLogic19 to find the deduction closure for all possible linear operators in just seconds. Our symbolic deduction engine is an intricate integration of DD and AR, which we apply alternately to expand the joint closure of known true state - ments until expansion halts. This process typically finishes within a few seconds to at most a few minutes on standard non-accelerator hardware. Algebraic reasoning There has not been a complete treatment for al...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "linear operators in seconds. Symbolic deduction engine an intricate integration of DD and AR, which applies alternately to expand joint closure of known truths until expansion halts. Process typically finishes within a few seconds to at most a few minutes on standard non-accelerator hardware....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "discovery and implemented in GeoLogic19 for both angle and ratios. We expanded this formulation to cover all reasoning about angles, ratios and distances between points and also arithmetic reasoning with geometric constants such as ‘pi’ or ‘1:2’ . Concrete examples of algebraic reasoning are given in Extended Data Table 2. On a high level, we first convert the input linear equations to a matrix of their coefficients. In particular, we create a coefficient matrix A ∈ RM×N in which N is the number...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "cover all reasoning about angles, ratios and distances between points and also arithmetic reasoning with geometric constants such as ‘pi’ or ‘1:2’ Concrete examples of algebraic reasoning are given in Extended Data Table 2. On a high level, we first convert the input linear equations to a matrix of their coefficients. In particular, we create a coefficient matrix A ∈ RM×N in which N is the number of variables and M is the number of input equations. Any equality is of the form a − b = c − d ⇔ a −...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "pair, representing a specific point on a specific line. Because all equalities are of the form ‘a − b − c + d = 0’ , we populate the row for each equality with values +1, −1, −1, +1 at columns corre - sponding to variables a , b, c and d . Running Gaussian elimination on A returns a new matrix with leading 1s at each of the columns, essen - tially representing each variable as a unique linear combination of all remaining variables. As an example, suppose we have ‘a − b = b − c ’, ‘d − c = a − d ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "1 -1 1 1 1 -1 1 1 1 1 1 1 -1 1 1 1 -1 1 1 1 1 1 1 -1 1 1 1 -1 1 1 1 1 1 -1 1 1 -1 1 1 1 -1 1 1 -1 1 1 -1 1 1 1 1 1 -1 1 1 -1 1 1 1 1 1 -1 1 1 1 -1 1 1 1 1 1 -1 1 1 -1 1 1 1 -1 1 1 1 1 -1 1 1 -1 1 1 1 -1 1 1 -1 1 1 1 -1 1 1 -1 1 1 -1 1 1 1 1 -1 1 1 -1 1 1 -1 1 1 1 -1 1 1 -1 1 1 -1 1 1 1 -1 1 1 -1 1 1 -1 1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -1 1 1 -...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "4-permutation of all variables. In the above Gaussian Elimination, for example, AR deduced that b = d from the three input equalities. To handle geometric constants such as ‘0.5 pi’ or ‘5:12’ , we included ‘pi’ and ‘1’ as default variables to all coefficient matrices. Deductive database implementation Unlike the original implementation of DD, we use a graph data structure to capture the symmetries of geometry, rather than using strings of canonical forms. With a graph data structure, we captured...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "to capture symmetries of geometry\n",
      "# Implicitly used deduction rules from geometric rule list\n",
      "# Explicitly stated rules for minimal proofs\n",
      "```\n",
      "\n",
      "## Deductive Database Implementation\n",
      "\n",
      "Unlike original implementation of DD, we use graph data structure to capture symmetries of geometry\n",
      "rather than using strings of canonical forms\n",
      "Captured not only symmetrical permutations of function arguments but also transitivity of equality, collinearity and concyclicity\n",
      "This graph data structure bakes into itself ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "step needs to be coupled with a traceback algorithm, which returns the minimal set of immediate ancestor statements that is necessary to deduce the conclusion statement of the step. This is the core building block for extracting proof graphs and minimal premises described in the main text. A minimal-premise-extraction algorithm is necessary to avoid superfluous auxiliary constructions that contribute to the proof through unnecessary transitivity. For example, ‘a = b ’ and ‘b = c ’ might not be n...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "y recording the following equalities:\n",
      "\n",
      "- a = b\n",
      "- b = c\n",
      "- c = d\n",
      "- a = d\n",
      "\n",
      "The graph is represented as a directed acyclic graph (DAG) with nodes a, b, c, and d, and edges [(a, b), (b, c), (c, d), (a, d)].\n",
      "\n",
      "Step: Traceback Algorithm\n",
      "The traceback algorithm performs a breadth-first search (BFS) on the equality transitivity graph to find the minimal set of immediate ancestor statements necessary to deduce the conclusion statement.\n",
      "\n",
      "The algorithm starts from the initial node a and explores all reachabl...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "shortest path of transitivity of equality between any pair of variables among a, b, c and d . For collinearity and concyclicity, however, the representation is more complex. In these cases, hypergraphs G (V, E) with 3-edges or 4-edges are used as the equality transitivity graph. The traceback is now equivalent to finding a minimum spanning tree (denoted MST in the following equation) for the target set S of nodes (three collinear nodes or four concyclic nodes) whose weight is the cardinality of ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "described in the previous sections and a target equa - tion with coefficients vector b ∈ RN, we determine the minimal set of premises for b by defining non-negative integer decision vectors x, y ∈ ZM and solve the following mixed-integer linear programming problem: ∑ xy xy Ax yb ,= min( +) s.t. (− )=xyii i ,T The minimum set of immediate parent nodes for the equality repre - sented by b will be the i th equations (i th rows in A ) whose corresponding decision value (xi − yi) is non-zero. Integra...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "col = 3)\n",
      "\n",
      "# Define the coefficients vector b\n",
      "b <- c(2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
      "\n",
      "# Initialize the variables\n",
      "x <- matrix(0, nrow = 9, ncol = 3)\n",
      "\n",
      "# Define the objective function coefficients\n",
      "c <- c(0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "\n",
      "# Define the constraint matrix\n",
      "X <- matrix(c(1, -1, 1), nrow = 9, ncol = 3)\n",
      "\n",
      "# Define the constraint coefficients\n",
      "y <- c(0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "\n",
      "# Define the coefficients for the equality constraints\n",
      "eq1 <- matrix(c(1, -1), nrow = 3, ncol = 1)\n",
      "eq2 <- matrix(c(1, -1), nrow...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "slope(AB) and ‘−1’ at the column of slope(CD). Gaussian elimination and mixed-integer linear programming is run again as AR executes, producing new equalities as inputs to the next iteration of DD. This loop repeats until the joint deduction closure stops expanding. Both DD and AR are deterministic processes that only depend on the theorem premises, therefore they do not require any design choices in their implementation. Proof pruning Although the set of immediate ancestors to any node is minim...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "r linear programming is run again as AR executes, producing new equalities as inputs to the next iteration of DD. This loop repeats until the joint deduction closure stops expanding. Both DD and AR are deterministic processes that only depend on the theorem premises, therefore they do not require any design choices in their implementation. Proof pruning Although the set of immediate ancestors to any node is minimal, this does not guarantee that the fully traced back dependency subgraph G(N) and ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "the auxiliary points and rerunning DD + AR on the smaller subset of premises to verify goal reachability. At the end, we return the minimum proof obtainable across all trials. This proof-pruning procedure is done both during synthetic data generation and after each successful proof search during test time. Parallelized data generation and deduplication We run our synthetic-data-generation process on a large number of parallel CPU workers, each seeded with a different random seed to reduce duplic...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ty. At the end, we return the minimum proof obtainable across all trials. This proof-pruning procedure is done both during synthetic data generation and after each successful proof search during test time. Parallelized data generation and deduplication We run our synthetic-data-generation process on a large number of parallel CPU workers, each seeded with a different random seed to reduce duplications. After running this process on 100,000 CPU workers for 72 hours, we obtained roughly 500 millio...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "We find no IMO-AG-30 problems in the synthetic data. On the set of geometry problems collected in JGEX17, which consists mainly of problems with moderate difficulty and well-known theorems, we find nearly 20 problems in the synthetic data. This suggests that the training data covered a fair amount of common knowledge in geometry, but the space of more sophisticated theorems is still much larger. Language model architecture and training We use the Meliad library35 for transformer training with it...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "7, which consists mainly of problems with moderate difficulty and well-known theorems, we find nearly 20 problems in the synthetic data. This suggests that the training data covered a fair amount of common knowledge in geometry, but the space of more sophisticated theorems is still much larger....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "embedding37. Sequence packing38,39 is also used because more than 90% of our sequences are under 200 in length. During training, a dropout40 rate of 5% is applied pre-attention and post-dense. A 4 × 4 slice of TPUv3 (ref. 41) is used as its hardware accelerator. For pre - training, we train the transformer with a batch size of 16 per core and a cosine learning-rate schedule that decays from 0.01 to 0.001 in 10,000,000 steps. For fine-tuning, we maintain the final learn - ing rate of 0.001 for an...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ngth. During training, a dropout rate of 5% is applied pre-attention and post-dense. A 4 × 4 slice of TPUv3 is used as its hardware accelerator. For pre-training, we train the transformer with a batch size of 16 per core and a cosine learning-rate schedule that decays from 0.01 to 0.001 in 10,000,000 steps. For fine-tuning, we maintain the final learning rate of 0.001 for another 1,000,000 steps. For the setup with no pretraining, we decay the learning rate from 0.01 to 0.001 in 1,000,000 steps....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "k options, using the score of each beam as its value function. This set-up is highly paral - lelizable across beams, allowing substantial speed-up when there are parallel computational resources. In our experiments, we use a beam size of k = 512, the maximum number of iterations is 16 and the branch - ing factor for each node, that is, the decoding batch size, is 32. This is the maximum inference-time batch size that can fit in the memory of a GPU V100 for our transformer size. Scaling up these ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "00 memory capacity, \n",
      "GPU workers per beam, \n",
      "GPU workers per problem, \n",
      "number of problems, \n",
      "pool size of CPU workers...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "the running time of the symbolic solver on each individual problem, which—by design—stays roughly constant across all beams. We use this and the language model decoding speed to infer the necessary parallelism needed for each problem, in isolation, to stay under different time limits at the IMO in Extended Data Fig. 1. The effect of data and search We trained AlphaGeometry on smaller fractions of the original training data (20%, 40%, 60% and 80%) and found that, even at 20% of training data, Alp...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "mains roughly constant across all beams design and the language model decoding speed to infer the necessary parallelism needed for each problem, in isolation, to stay under different time limits at the IMO...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "problems. A similar result of 21 problems can be obtained by reducing the search depth from 16 to only two, while keeping the beam size constant at 512. Evaluation on a larger test set We evaluated AlphaGeometry and other baselines on a larger test set of 231 geometry problems, curated in ref. 17 . This set covers a wider range of sources outside IMO competitions: textbook examples and exercises, regional olympiads and famous geometry theorems; some are even more complex than typical IMO problem...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "nly two, while keeping the beam size constant at 512. Evaluation on a larger test set We evaluated AlphaGeometry and other baselines on a larger test set of 231 geometry problems, curated in ref. 17. This set covers a wider range of sources outside IMO competitions: textbook examples and exercises, regional olympiads and famous geometry theorems; some are even more complex than typical IMO problems, such as the five circles theorem, Morley’s theorem or Sawayama and Thébault’s theorem. The overal...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "neuro-symbolic set-up lies in its ability to generate auxiliary constructions, which is an important ingredient across many mathematical domains. In Extended Data Table 3, we give examples in four other mathematical domains in which coming up with auxiliary constructions is key to the solution. In Extended Data Table 4, we give a line-by-line comparison of a geometry proof and an inequality proof for the IMO 1964 Problem 2, highlighting how they both fit into the same framework. Our paper shows ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "uxiliary constructions, which is an important ingredient across many mathematical domains. In Extended Data Table 3, we give examples in four other mathematical domains in which coming up with auxiliary constructions is key to the solution. In Extended Data Table 4, we give a line-by-line comparison of a geometry proof and an inequality proof for the IMO 1964 Problem 2, highlighting how they both fit into the same framework. Our paper shows that language models can learn to come up with auxiliar...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "implementation (1). (4) A traceback procedure for the symbolic engine. Using these four ingredients and the algorithm described in the main text, one can generate synthetic data for any target domain. As shown in our paper, there are non-trivial engineering challenges in building each ingredient. For example, current formalizations of combinatorics are very nascent, posing challenges to (1) and (2). Also, building pow - erful symbolic engines for different domains requires deep domain expertise,...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "rate synthetic data for any target domain. As shown in our paper, the development of such an engine is challenging, particularly when it comes to formalizing combinatorics and building powerful symbolic engines for different domains. However, we consider applying this framework to tackle these challenges in the future.\n",
      "\n",
      "Transformer-based theorem proving is a long-standing approach in automated theorem proving, dating back to the 1950s, with notable results in first-order logic solvers like E and...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "demonstrating great successes in premise selection and proof guid - ance46–49, as well as SAT solving50. On the other hand, transformer18 exhibits outstanding reasoning capabilities across a variety of tasks51–53. The first success in applying transformer language models to theorem proving is GPT-f (ref. 15 ). Its follow up extensions2,16 further developed this direction, allowing machines to solve some olympiad-level prob - lems for the first time. Innovation in the proof-search algorithm and o...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ng50. On the other hand, transformer18 exhibits outstanding reasoning capabilities across a variety of tasks51–53. The first success in applying transformer language models to theorem proving is GPT- (ref. 15). Its follow-up extensions2, further developed this direction, allowing machines to solve some Olympiad-level problems for the first time. Innovation in the proof-search algorithm and online training3 also improves transformer-based methods, solving a total of ten (adapted) IMO problems in ...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "largely considered solved since the introduction of Wu’s method21, which can theoretically decide the truth value of any geometrical statement of equality type, building on specialized algebraic tools introduced in earlier works54,55. Even though com - puter algebra has strong theoretical guarantees, its performance can be limited in practice owing to their large time and space com - plexity56. Further, the methodology of computer algebra is not of interest to AI research, which instead seeks to...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "truth value of any geometrical statement of equality type, building on specialized algebraic tools introduced in earlier works. Even though computer algebra has strong theoretical guarantees, its performance can be limited in practice owing to their high complexity and computational overhead56. Further, the methodology of computer algebra is not of interest to AI research, which instead seeks to prove theorems using search methods, a more human-like and general-purpose process....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "proving of today, however, is still relying on human-designed heuristics for auxiliary constructions10–14. Geometry theorem proving falls behind the recent advances made by machine learning because its presence in formal mathematical libraries such as Lean31 or Isabelle62 is extremely limited. Synthetic data in theorem proving. Synthetic data has long been recognized and used as an important ingredient in theorem prov - ing63–66. State-of-the-art machine learning methods make use of expert itera...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "rem proving falls behind the recent advances made by machine learning because its presence in formal mathematical libraries such as Lean or Isabelle is extremely limited. Synthetic data in theorem proving. Synthetic data has long been recognized and used as an important ingredient in theorem proving63–66....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "on existing conjectures curated by humans and does not learn from proof attempts on the target theo - rems. Their approach is thus orthogonal and can be used to further improve AlphaGeometry. Most similar to our work is Firoiu et al.69, whose method uses a forward proposer to generate synthetic data by depth-first exploration and trains a neural network purely on these synthetic data. Our work, on the other hand, uses breadth-first explora - tion, necessary to obtain the minimal proofs and premi...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Our approach is orthogonal to it, and can be used to further improve AlphaGeometry. Most similar to our work is Firoiu et al.69, whose method uses a forward proposer to generate synthetic data by depth-first exploration and trains a neural network purely on these synthetic data. Our approach, on the other hand, uses breadth-first exploration, necessary to obtain the minimal proofs and premises, and uses a traceback algorithm to identify auxiliary constructions, thus introducing new symbols and h...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "Proc. 28th International Conference on Automated Deduction, CADE 28 (eds Platzer, A. & Sutcliffe, G.) 625–635 (Springer, 2021). 32. Krueger, R., Han, J. M. & Selsam, D. in Proc. 28th International Conference on Automated Deduction, CADE 28 (eds Platzer, A. & Sutcliffe, G.) 577–588 (Springer, 2021). 33. de Moura, L. & Bjørner, N. in Proc. 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS 2008 (eds Ramakrishnan, C. R. & Rehof, J.) 337–340 (Sp...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "G.) 625–635 (Springer, 2021) 32. Krueger, R., Han, J. M. & Selsam, D. in Proc. 28th International Conference on Automated Deduction, CADE 28 (eds Platzer, A. & Sutcliffe, G.) 577–588 (Springer, 2021) 33. de Moura, L. & Bjørner, N. in Proc. 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS 2008 (eds Ramakrishnan, C. R. & Rehof, J.) 337–340 (Springer, 2008) 34. Todd, P. A method for the automated discovery of angle theorems. EPTCS 352, 148–15...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 5485–5551 (2020). 38. Kosec, M., Fu, S. & Krell, M. M. Packing: towards 2x NLP BERT acceleration. Preprint at https://openreview.net/forum?id=3_MUAtqR0aA (2021). 39. Krell, M. M., Kosec, M., Perez, S. P. & Iyer, M., Fitzgibbon A. W. Efficient sequence packing without cross-contamination: accelerating large language models without impacting performance. Preprint at https://arxiv.org/abs/2107.02027 (2022). 40. Srivastava, N., Hinton...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Krell, M. M. Packing: towards 2x NLP BERT acceleration. Preprint at https://openreview.net/forum?id=3_MUAtqR0aA (2021). 39. Krell, M. M., Kosec, M., Perez, S. P. & Iyer, M., Fitzgibbon A. W. Efficient sequence packing without cross-contamination: accelerating large language models without impacting performance. Preprint at https://arxiv.org/abs/2107.02027 (2022). 40. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout: a simple way to prevent neural networks fro...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "ACM. 7, 201–215 (1960). 44. Schulz, S. E – a brainiac theorem prover. AI Commun. 15, 111–126 (2002). 45. Riazanov, A. & Voronkov, A. in Proc. First International Joint Conference on Automated Reasoning, IJCAR 2001 (eds Goré, R., Leitsch, A. & Nipkow, T.) 376–380 (Springer, 2001). 46. Irving, G. et al. DeepMath - deep sequence models for premise selection. Adv. Neural Inf. Process. Syst. https://doi.org/10.48550/arXiv.1606.04442 (2016). 47. Wang, M., Tang, Y., Wang, J. & Deng, J. Premise selectio...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "5. Riazanov, A. & Voronkov, A. in Proc. First International Joint Conference on Automated Reasoning, IJCAR 2001 (eds Goré, R., Leitsch, A. & Nipkow, T.) 376–380 (Springer, 2001). 46. Irving, G. et al. DeepMath - deep sequence models for premise selection. Adv. Neural Inf. Process. Syst. https://doi.org/10.48550/arXiv.1606.04442 (2016). 47. Wang, M., Tang, Y., Wang, J. & Deng, J. Premise selection for theorem proving by deep graph embedding. Adv. Neural Inf. Process. Syst. https://doi.org/10.4855...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "https://doi. org/10.48550/arXiv.1802.03685 (2019). 51. Saxton, D., Grefenstette, E., Hill, F. & Kohli, P. Analysing mathematical reasoning abilities of neural models. Preprint at https://doi.org/10.48550/arXiv.1904.01557 (2019). 52. Lample, G. & Charton F. Deep learning for symbolic mathematics. Preprint at https://doi. org/10.48550/arXiv.1912.01412 (2019). 53. Charton, F., Hayat, A. & Lample, G. Learning advanced mathematical computations from examples. Preprint at https://doi.org/10.48550/arXi...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "eural models.\n",
      "\n",
      "Preprint at https://doi.org/10.48550/arXiv.1802.03685 (2019)\n",
      "\n",
      "Lample, G. & Charton F. Deep learning for symbolic mathematics.\n",
      "\n",
      "Preprint at https://doi.org/10.48550/arXiv.1912.01412 (2019)\n",
      "\n",
      "Charton, F., Hayat, A. & Lample, G. Learning advanced mathematical computations from examples.\n",
      "\n",
      "Preprint at https://doi.org/10.48550/arXiv.2006.06462 (2021)\n",
      "\n",
      "Collins, G. E. in Proc. 2nd GI Conference on Automata Theory and Formal Languages (ed. Barkhage, H.) 134–183 (Springer, 1975)\n",
      "\n",
      "Ritt, J. F....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "reasoning in geometry theorem proving with Prolog. J. Autom. Reason. 2, 329–390 (1986). 59. Quaife, A. Automated development of Tarski’s geometry. J. Autom. Reason. 5, 97–118 (1989). 60. McCharen, J. D., Overbeek, R. A. & Lawrence, T. in The Collected Works of Larry Wos 166–196 (2000). 61. Chou, S. C., Gao, X. S. & Zhang, J. Machine Proofs in Geometry: Automated Production of Readable Proofs for Geometry Theorems (World Scientific, 1994). 62. Paulson, L. C. (ed.) Isabelle: A Generic Theorem Prov...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "dynamic polynomial proofs. Adv. Neural Inf. Process. Syst. https://doi.org/10.48550/arXiv.1906.01681 (2019). 66. Wang, M. & Deng, J. Learning to prove theorems by learning to generate theorems. Adv. Neural Inf. Process. Syst. 33, 18146–18157 (2020). 67. Aygün, E. et al. in Proc. 39th International Conference on Machine Learning 1198–1210 (PMLR, 2022). 68. Andrychowicz, M. et al. Hindsight experience replay. Adv. Neural Inf. Process. Syst. https://doi.org/10.48550/arXiv.1707.01495 (2017). 69. Fir...\n",
      "\n",
      "PROCESSED TEXT:\n",
      ". https://doi.org/10.48550/arXiv.1906.01681 (2019). Learning to prove theorems by learning to generate theorems. Adv. Neural Inf. Process. Syst. 33, 18146–18157 (2020). In Proc. 39th International Conference on Machine Learning 1198–1210 (PMLR, 2022). Hindsight experience replay. Adv. Neural Inf. Process. Syst. https://doi.org/10.48550/arXiv.1707.01495 (2017). Training a first-order theorem prover from synthetic data....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "and P. Bak. Author contributions T.H.T. conceived the project, built the codebase, carried out experiments, requested manual evaluation from experts and drafted the manuscript. Y.W. advocated for the neuro-symbolic setting and advised on data/training/codebase choices. Q.V.L. advised on scientific methodology and revised the manuscript. H.H. advised on scientific methodology, experimental set-ups and the manuscript. T.L. is the PI of the project, advised on model designs/implementations/experime...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "performing experiments, and drafting the manuscript. Y.W. contributed to the neuro-symbolic setting, data/training/codebase choices, and manuscript evaluation. Q.V.L. provided scientific methodology, revised the manuscript, and advised on data/training/codebase choices. H.H. contributed to scientific methodology, experimental set-ups, and manuscript structure and writing. T.L. is the project PI, advised on model designs, implemented, and wrote the manuscript....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "materials should be addressed to Trieu H. Trinh or Thang Luong. Peer review information Nature thanks the anonymous reviewers for their contribution to the peer review of this work. Reprints and permissions information is available at http://www.nature.com/reprints. Extended Data Fig. 1 | The minimum number of parallel CPU workers to solve all 25 problems and stay under the time limit, given four parallel copies of the GPU V100-accelerated language model. Each problem has a different running tim...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "t https://www.nature.com/reprints \n",
      "\n",
      "Reprints and permissions information available at http://www.nature.com/reprints \n",
      "\n",
      "Extended Data Fig. 1 The minimum number of parallel CPU workers to solve all 25 problems and stay under the time limit given four parallel copies of the GPU V100-accelerated language model each problem has a different running time resulting from unique size of the deduction closure we observed that running time does not correlate with the difficulty of the problem...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "M and N through O. AlphaGeometry constructs point K to materialize this axis, whereas humans simply use the existing point R for the same purpose. This is a case in which proof pruning itself cannot remove K and a sign of similar redundancy in our synthetic data. To prove five-point concyclicity, AlphaGeometry outputs very lengthy, low-level steps, whereas humans use a high-level insight (OR is the symmetrical axis of both LN and AM) to obtain a broad set of conclusions all at once. For algebrai...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ndicating redundancy in the system, which can be addressed by redefining the point R to serve as the axis of symmetry. This can be achieved by introducing a new variable, S, to represent the symmetrical axis of both LN and AM. This can be done by introducing the following equation:\n",
      "\n",
      "S = (P - R) / 2\n",
      "\n",
      "This equation allows for the derivation of the position of point S, which can then be used to prove the five-point concyclicity of the existing point R....\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "requirement. Extended Data Fig. 3 | Side-by-side comparison of human proof and AlphaGeometry proof for the IMO 2000 P6. This is a harder problem (average human score = 1.05/7), with a large number of objects in the problem statements, resulting in a very crowded diagram. Left, the human solution uses complex numbers. With a well-chosen coordinate system, the problem is greatly simplified and a solution follows naturally through algebraic manipulation. Right, AlphaGeometry solution involves two a...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "phaGeometry proof for the IMO 2000 P6. This is a harder problem (average human score = 1.05/7), with a large number of objects in the problem statements, resulting in a very crowded diagram. Human solution uses complex numbers. A well-chosen coordinate system greatly simplifies the problem, allowing a solution to follow naturally through algebraic manipulation. AlphaGeometry solution involves two auxiliary constructions and more than 100 deduction steps, with many low-level steps that are tediou...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "of low-level deductions into fewer proof steps. Article Extended Data Fig. 4 | Side-by-side comparison of human proof and AlphaGeometry proof for the IMO 2019 P2. This is one out of five unsolved problems by AlphaGeometry. Left, the human solution uses both auxiliary constructions and barycentric coordinates. With a well-chosen coordinate system, a solution becomes available through advanced algebraic manipulation. Right, AlphaGeometry solution when provided with the ground-truth auxiliary const...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "n of human proof and AlphaGeometry proof for the IMO 2019 P2. This is one out of five unsolved problems by AlphaGeometry. Left, the human solution uses both auxiliary constructions and barycentric coordinates. With a well-chosen coordinate system, a solution becomes available through advanced algebraic manipulation. Right, AlphaGeometry solution when provided with the ground-truth auxiliary construction for a synthetic proof. This auxiliary construction can be found quickly with the knowledge of...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "factor of 3. Extended Data Fig. 5 | Human proof for the IMO 2008 P6. This is an unsolved problem by AlphaGeometry and also the hardest one among all 30 problems, with an average human score of only 0.28/7. This human proof uses four auxiliary constructions (diameters of circles W1 and W2) and high-level theorems such as the Pitot theorem and the notion of homothety. These high-level concepts are not available to our current version of the symbolic deduction engine both during synthetic data gene...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "phaGeometry and the hardest one among all 30 problems, with an average human score of only 0.28/7 This human proof uses four auxiliary constructions (diameters of circles W1 and W2) and high-level theorems such as the Pitot theorem and the notion of homothety These high-level concepts are not available to our current version of the symbolic deduction engine both during synthetic data generation and proof search Supplying AlphaGeometry with the auxiliary constructions used in this human proof als...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "test-time performance of AlphaGeometry. Article Extended Data Fig. 6 | Analysis of AlphaGeometry performance under changes made to its training and testing. a, The effect of reducing training data on AlphaGeometry performance. At 20% of training data, AlphaGeometry still solves 21 problems, outperforming all other baselines. b , Evaluation on a larger set of 231 geometry problems, covering a diverse range of sources outside IMO competitions. The rankings of different machine solvers stays the sa...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ance under changes made to its training and testing\n",
      "a, The effect of reducing training data on AlphaGeometry performance\n",
      "At 20% of training data, AlphaGeometry still solves 21 problems, outperforming all other baselines\n",
      "b, Evaluation on a larger set of 231 geometry problems, covering a diverse range of sources outside IMO competitions\n",
      "The rankings of different machine solvers stays the same as in Table 1, with AlphaGeometry solving almost all problems\n",
      "c, The effect of reducing beam size during t...\n",
      "==========================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "construct the random premises These actions include constructions to create new points that are related to others in a certain way, for example, collinear, incentre/excentre etc., and constructions that take a number as its parameter. ArticleExtended Data Table 2 | Three examples of algebraic reasoning (AR) in geometry theorem proving, with AR proof steps between the two tags <AR></AR> In AlphaGeometry, the engine AR can execute all three examples efficiently, under a unified procedure of Gaussi...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Arithmetic Reasoning in Geometry Theorems**\n",
      "----------------------------------------\n",
      "\n",
      "Arithmetic reasoning plays a crucial role in geometry, enabling us to derive important theorems from basic properties. In this section, we will explore three examples of algebraic reasoning in geometry theorems.\n",
      "\n",
      "**Algebraic Reasoning in Geometric Theorems**\n",
      "--------------------------------------------\n",
      "\n",
      "Algebraic reasoning is a fundamental aspect of geometry, allowing us to manipulate and simplify geometric exp...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "capable of (1) algebraic rewrites and simplification and (2) applying the inequality rule of arithmetic means–geometric means. With the original premises, directly applying AM-GM fails to deliver a solution, which is similar to the geometry example, for which DD + AR fails to solve the simple problem. Some correct auxiliary constructions are necessary for both symbolic engines (DD + AR in the case of geometry and AM-GM in the case of inequality) to succeed, as shown in the last two rows of the t...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "etic-geometric mean rule, and the inequality rule of arithmetic means-geometric mean. Directly applying these rules fails to deliver a solution to the given problem, similar to the geometry example, for which the addition of digits (DD) fails to solve the simple problem....\n",
      "==========================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "processed_text = \"\" #Initialize processed_text\n",
    "with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "    for chunk_num, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
    "        # Process chunk and append to complete text\n",
    "        processed_chunk = process_chunk(chunk, chunk_num)\n",
    "        processed_text += processed_chunk + \"\\n\"\n",
    "        \n",
    "        # Write chunk immediately to file\n",
    "        out_file.write(processed_chunk + \"\\n\")\n",
    "        out_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cffe8d",
   "metadata": {},
   "source": [
    "Let's print out the final processed versions to make sure things look good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89ef51a7-f13f-49a4-8f73-9ac8ce75319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "Input file: ./resources/extracted_text.txt\n",
      "Output file: clean_extracted_text.txt\n",
      "Total chunks processed: 101\n",
      "\n",
      "Preview of final processed text:\n",
      "\n",
      "BEGINNING:\n",
      "ng, owing to the difficulty among world's best talents in pre-university mathematics Current machine-learning approaches are not applicable to most mathematical domains due to high cost of translating human proofs into machine-verifiable format The problem is worse for geometry due to unique translation challenges resulting in scarcity of training data We propose AlphaGeometry a theorem prover for Euclidean plane geometry that synthesizes millions of theorems and proofs across different levels of complexity\n",
      "anching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theore\n",
      "\n",
      "...\n",
      "\n",
      "END:\n",
      "Assume ABC is a triangle\n",
      "2. Let AB = b and BC = c\n",
      "3. Show that the area of triangle ABC is given by (b x c) / 2\n",
      "4. Show that the area of triangle ABC is equal to (b x c) / 2\n",
      "\n",
      "**Auxiliary Constructions in Geometry**\n",
      "-----------------------------------------\n",
      "\n",
      "Auxiliary constructions play a crucial role in geometry proofs, enabling us to establish important relationships between geometric objects. In this section, we will examine four examples of auxiliary constructions in different domains.\n",
      "\n",
      "**Examples of Auxiliary Constructions in Different Domains**\n",
      "---------------------------------------------------------\n",
      "\n",
      "In this section, we will explore four examples of auxiliary constructions in different domains.\n",
      "\n",
      "1. **Domain: 2D\n",
      "etic-geometric mean rule, and the inequality rule of arithmetic means-geometric mean. Directly applying these rules fails to deliver a solution to the given problem, similar to the geometry example, for which the addition of digits (DD) fails to solve the simple problem.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Input file: {INPUT_FILE}\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(f\"Total chunks processed: {num_chunks}\")\n",
    "\n",
    "# Preview the beginning and end of the complete processed text\n",
    "print(\"\\nPreview of final processed text:\")\n",
    "print(\"\\nBEGINNING:\")\n",
    "print(processed_text[:1000])\n",
    "print(\"\\n...\\n\\nEND:\")\n",
    "print(processed_text[-1000:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
